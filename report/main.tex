\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}

\begin{document}

\title{Detecting AI-Generated Speech:\\Synthetic Voice Classification using Deep Learning}

\author{
\IEEEauthorblockN{Giuseppe Doda}
\IEEEauthorblockA{\textit{University of Zurich}\\
giuseppe.doda@uzh.ch}
\and
\IEEEauthorblockN{Ronald Domi}
\IEEEauthorblockA{\textit{University of Zurich}\\
ronald.domi@uzh.ch}
\and
\IEEEauthorblockN{Arjun Roy}
\IEEEauthorblockA{\textit{University of Zurich}\\
arjun.roy@uzh.ch}
}

\maketitle

\begin{abstract}
The rapid advancement of text-to-speech (TTS) and voice conversion technologies has increased the realism of synthetic speech \cite{articletts}, raising security risks related to fraud, identity theft, and misinformation. This project addresses the challenge of distinguishing between bona fide (human) and spoofed (AI-generated) audio by implementing, evaluating and comparing three distinct deep learning architectures. We approach this classification problem from two perspective, image based models and time series models. The three models developed are: Convolutional Neural Network (CNN), Vision Transformer (ViT) and Long Short-Term Memory (LSTM) network. We utilize the ASVspoof 2021 dataset with labeled bona-fide or spoof audios to train and assess these models. Finally, we compare the results given by the three models used in the project.
\end{abstract}

\section{Introduction}

The rapid development of synthetic speech generation has evolved greatly in recent years. While these technologies offer significant benefits for accessibility and entertainment, their ability to clone voices from as little as a few seconds of reference audio introduces its own set of problems. These tools have also the potential to be used for malicious actors to commit fraud, spread misinformation, and impersonate individuals through voice spoofing attacks.

The ASVspoof challenge series \cite{asvspoof2021} has been very useful by providing standardized datasets and evaluation protocols where different models can be trained and evaluated in spoof-detection.

We formulate the core problem as a binary classification task: given an input audio sample, the system must determine whether it was produced by a human voice or synthesized by an algorithm.


\section{Related Work}

Voice spoofing detection has been extensively studied. Traditional approaches relied on hand-crafted features such as linear frequency cepstral coefficients (LFCCs) combined with classical machine learning classifiers. However, deep learning approaches have demonstrated superior performance in recent ASVspoof challenges.

Convolutional Neural Networks \cite{inproceedingscnn} have been successfully applied to spectrogram-based analysis, treating audio classification as a computer vision problem. Vision Transformers \cite{dosovitskiy2021imageworth16x16words}, which have revolutionized image classification, have recently been adapted for audio tasks with promising results. Recurrent neural networks, particularly LSTMs, working on the raw audio time-series \cite{articlernn} have also proved to be very competitive to the image based networks.

\section{Methodology}

\subsection{Dataset}

We utilize the ASVspoof 2021 Logical Access (LA) dataset \cite{asvspoof2021}, which contains audio samples of both bonafide human speech and synthetic speech. The datasets main concern is the imbalanced class representation, with approximately 90\% spoof samples and only 10\% bonafide samples.

To combat this particular problem, the pre-processing pipeline will use stratified split, so that all the training, validation and test sets have the same class distribution. 
In addition, we implement class weights, so we weigh bonafide 9x higher compared to spoof to compensate for class imbalance.

All audio files are sampled at 16 kHz. After studying the duration of all the samples on the dataset, we decided to crop or pad to 4 seconds. This makes the rest of the pipeline easier to work with, as handling variable input size takes more effort. 4 seconds, or 200 frames captures the 95th percentile or all the audio sample lengths. 

The data after padding, cropping is normalized and augmented before being used by the models.

\subsection{Feature Extraction}

We employ two distinct feature representations based on whether we will use an image based model or the time-series representation. 

\subsubsection{Mel-Spectrograms}

For the CNN and Vision Transformer models, we convert raw audio waveforms into log-scale Mel-spectrograms. The spectrograms are computed with the following parameters:

\begin{itemize}
    \item Number of Mel bands: 128
    \item FFT size: 2048
    \item Hop length: 512 samples
    \item Frequency range: 20 Hz to 8000 Hz
\end{itemize}

The resulting spectrograms are treated as single-channel grayscale images with dimensions $128 \times T$, where $T$ is the number of time frames (here 200). Each spectrogram is min-max normalized to the range $[0, 1]$ to facilitate neural network training.

\subsubsection{MFCCs}

For the Bi-LSTM model, we extract Mel-frequency cepstral coefficients (MFCCs). We compute 13 static MFCC coefficients. For the sake of simplicity, we did not include first-order (delta) and second-order (delta-delta) derivatives, resulting in 39 features per time frame, in our implementation.

The MFCC features are normalized per coefficient across the time dimension to have zero mean and unit variance. 

\subsection{Data Augmentation}

To improve model generalization and prevent overfitting, we apply different augmentation strategies tailored to each feature representation:

\subsubsection{Spectrogram Augmentation}

For the CNN and ViT models operating on Mel-spectrograms, we apply SpecAugment:

\begin{itemize}
    \item \textbf{Time masking}: Random masking of up to 30 consecutive time frames
    \item \textbf{Frequency masking}: Random masking of up to 20 consecutive frequency bins
    \item \textbf{Time Shifting}: Random move the audio in the time dimension, looping back the audio when passing the 200 frames limit.
\end{itemize}

Other augmentation techniques like adding noise, time stretching an volume gain were note implemented for sake of simplicity.
Since we used fixed padding as the input, we skipped crop augmentation. It is important to note that this augmentation is done before normalization. 


\subsubsection{Audio Augmentation}

For the Bi-LSTM model processing raw audio features, we apply:

\begin{itemize}
    \item \textbf{Gaussian noise injection}: Addition of random Gaussian noise with amplitude factor 0.005
    \item \textbf{Volume scaling}: Scaling the volume (0.8 to 1.2)
    \item \textbf{Time shift}: The maximum shift the audio can have is 10\% of the length
\end{itemize}

This augmentation simulates realistic recording conditions and microphone artifacts.

\subsection{Model Architectures}

\subsubsection{CNN with Attention}

Our CNN architecture consists of four convolutional blocks, each containing:

\begin{itemize}
    \item 2D convolution (kernel size $3 \times 3$, stride 1, padding 1)
    \item Batch normalization
    \item ReLU activation
    \item Max pooling ($2 \times 2$)
\end{itemize}

The number of channels progresses as: $1 \rightarrow 64 \rightarrow 128 \rightarrow 256 \rightarrow 512$.

Global average pooling reduces spatial dimensions, and a dropout layer (rate 0.3) provides regularization. Finally, a linear classifier produces class logits.

\subsubsection{Vision Transformer}

Our Vision Transformer (ViT) architecture adapts the transformer paradigm to spectrogram classification. The key components are:

\begin{itemize}
    \item \textbf{Patch embedding}: The input spectrogram is divided into non-overlapping $16 \times 16$ patches, which are linearly embedded into 768-dimensional vectors
    \item \textbf{Position encoding}: Learnable position embeddings are added to preserve spatial structure
    \item \textbf{Class token}: A learnable CLS token is prepended to the sequence
    \item \textbf{Transformer encoder}: 6 transformer layers with 8 attention heads each
    \item \textbf{Classification head}: A linear layer maps the final CLS token representation to class logits
\end{itemize}

Each transformer layer consists of multi-head self-attention followed by a feed-forward network (MLP ratio 4.0), with layer normalization and residual connections. Dropout (rate 0.1) is applied for regularization.

The self-attention mechanism allows the model to capture global dependencies across the entire spectrogram, potentially identifying subtle patterns that span multiple frequency bands and time frames.

\subsubsection{Bidirectional LSTM}

Our recurrent architecture processes the sequential MFCC features through:

\begin{itemize}
    \item \textbf{Input projection}: Linear layer mapping 13 MFCC features to 256 dimensions, followed by layer normalization, ReLU activation, and dropout
    \item \textbf{Bidirectional LSTM}: 2 layers with hidden size 256, processing sequences in both forward and backward directions (effective hidden size 512)
    \item \textbf{Attention mechanism}: Temporal attention aggregates frame-level representations into a fixed-size context vector
    \item \textbf{Classification head}: Two-layer MLP with dropout, mapping the context vector to class logits
\end{itemize}

\subsection{Handling Class Imbalance}

We address the class imbalance through weighted cross-entropy loss:

\begin{equation}
\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} w_{y_i} \log(\hat{y}_i)
\end{equation}

where $w_{y_i}$ is the weight for the true class of sample $i$. We set $w_{\text{bonafide}} = 5.0$ and $w_{\text{spoof}} = 0.56$, forcing the model to pay substantially more attention to bonafide samples, while also preventing it from completely disregarding the majority spoof class.

\subsection{Training Configuration}

All models are trained with consistent hyperparameters to ensure fair comparison:

\begin{itemize}
    \item \textbf{Optimizer}: Adam with learning rate 0.001 and weight decay $10^{-4}$
    \item \textbf{Batch size}: 32
    \item \textbf{Maximum epochs}: 50
    \item \textbf{Learning rate scheduler}: ReduceLROnPlateau with patience 5 and factor 0.5
    \item \textbf{Early stopping}: Patience of 10 epochs based on validation loss
\end{itemize}

\section{Experimental Setup}

\subsection{Evaluation Metrics}

Given the class imbalance, we report multiple metrics beyond simple accuracy:

\begin{itemize}
    \item \textbf{Accuracy}: Overall classification accuracy
    \item \textbf{Precision}: Proportion of true positives among predicted positives
    \item \textbf{Recall}: Proportion of true positives among actual positives
    \item \textbf{F1-Score}: Harmonic mean of precision and recall
\end{itemize}

We compute both macro-averaged metrics (treating both classes equally) and per-class metrics to assess performance on both bonafide and spoof samples.

\section{Results}

\subsection{Quantitative Performance}

Table \ref{tab:performance} presents the overall performance of all three models on the test set. All metrics are computed after training convergence with early stopping.

\begin{table}[t]
\centering
\caption{Performance comparison of the three architectures on the test set. Results show macro-averaged metrics.}
\label{tab:performance}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} \\
\midrule
CNN + Attn & 0.9891 & 0.9892 & 0.9891 & 0.9891 \\
ViT & 0.8974 & 0.8053 & 0.8974 & 0.8488 \\
Bi-LSTM & 0.9657 & 0.9666 & 0.9657 & 0.9661 \\
\bottomrule
\end{tabular}
\end{table}

% \textit{Note: Performance metrics will be populated after training completion. Placeholder dashes indicate pending experimental results.}

\subsection{Per-Class Performance}

Table \ref{tab:per_class} breaks down the performance by class, which is particularly important given the class imbalance.

We do acknowledge issues with the ViT model as it was too large, data for bonafide was inadequate, and additionally we lacked sufficient compute resources to train on a larger scale. It might have been possible to train a smaller model but with inadequate compute resources we cannot be sure it will have worked.

\begin{table}[t]
\centering
\caption{Per-class performance metrics. Due to class imbalance, per-class analysis is crucial for understanding model behavior.}
\label{tab:per_class}
\begin{tabular}{llccc}
\toprule
\textbf{Model} & \textbf{Class} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} \\
\midrule
\multirow{2}{*}{CNN} & Bonafide & 0.9429 & 0.9516 & 0.9472 \\
                     & Spoof & 0.9945 & 0.9934 & 0.9939 \\
\midrule
\multirow{2}{*}{ViT} & Bonafide & 0.0000 & 0.0000 & 0.0000 \\
                     & Spoof & 0.8974 & 1.0000 & 0.9459 \\
\midrule
\multirow{2}{*}{Bi-LSTM} & Bonafide & 0.8162 & 0.8594 & 0.8373 \\
                         & Spoof & 0.8974 & 0.9779 & 0.9808 \\
\bottomrule
\end{tabular}
\label{tab:per_class}
\end{table}

\subsection{Training Dynamics}

\begin{figure}[t]
\centering
\begin{subfigure}{0.48\columnwidth}
\includegraphics[width=\textwidth]{images/cnn_training_curve.jpeg}
\caption{CNN}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\columnwidth}
\includegraphics[width=\textwidth]{images/Vit_training_curve.jpeg}
\caption{ViT}
\end{subfigure}
\vspace{0.5em}
\begin{subfigure}{0.48\columnwidth}
\centering
\includegraphics[width=\textwidth]{images/rnn_training_curve.jpeg}
\caption{Bi-LSTM}
\end{subfigure}
\caption{Training and validation loss curves for all three models. Early stopping prevents overfitting by monitoring validation performance.}
\label{fig:training_curves}
\end{figure}

Figure \ref{fig:training_curves} shows the training and validation loss curves. The convergence behavior varies across architectures, with the CNN typically converging faster due to its smaller parameter count, while the ViT requires more epochs to fully leverage its representational capacity.

\subsection{Confusion Matrix}

\begin{figure}[t]
\centering
\includegraphics[width=0.7\columnwidth]{images/confusion_matrix.jpeg}
\caption{Confusion matrix showing the classification performance across all models. Rows represent true labels, columns represent predictions.}
\label{fig:confusion_matrix}
\end{figure}

Figure \ref{fig:confusion_matrix} presents the confusion matrix for the models. This matrix reveals whether models exhibit bias toward the majority class (spoof) and their ability to correctly identify the minority class (bonafide).

\subsection{Performance Metrics Visualization}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{images/metrics.jpeg}
\caption{Comparison of performance metrics (Accuracy, Precision, Recall, F1-Score) across all three architectures.}
\label{fig:metrics}
\end{figure}

Figure \ref{fig:metrics} provides a visual comparison of the key performance metrics across all three architectures, highlighting the relative strengths of each model.  

\subsection{Recall Analysis}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{images/recall.jpeg}
\caption{Per-class recall comparison across the three models, showing performance on bonafide and spoof samples separately.}
\label{fig:recall}
\end{figure}

Figure \ref{fig:recall} shows the per-class recall for each model, which is particularly important given the class imbalance in the dataset.

\subsection{ROC Curve Analysis}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{images/ROC.jpeg}
\caption{Receiver Operating Characteristic (ROC) curves for all three models, illustrating the trade-off between true positive rate and false positive rate.}
\label{fig:roc}
\end{figure}

Figure \ref{fig:roc} displays the ROC curves for all three architectures. The area under the curve (AUC) provides a single-number summary of model performance across all classification thresholds.

\section{Discussion}

\subsection{Architectural Insights}

The comparison of three distinct architectures provides insights into the nature of synthetic speech artifacts:

\paragraph{CNN Performance} The convolutional approach treats spoofing detection as a visual pattern recognition problem. The attention mechanism enhances performance by allowing the model to focus on localized artifacts in the spectrogram. This architecture is particularly effective when synthetic speech contains visible spectral discontinuities or unnatural patterns.

\paragraph{ViT Performance} The Vision Transformer's self-attention mechanism enables modeling of long-range dependencies across the spectrogram. This global perspective can capture subtle correlations between distant frequency bands and time frames that might be missed by local convolutions. However, this comes at the cost of increased computational requirements and potentially slower convergence.

\paragraph{Bi-LSTM Performance} The recurrent architecture operates on MFCC features, modeling the temporal dynamics of speech production. The bidirectional processing captures both past and future context, while the attention mechanism identifies critical time frames. This approach is effective at detecting temporal anomalies in synthetic speech, such as unnatural prosody or irregular timing patterns.

\subsection{Class Imbalance Impact}

The severe class imbalance (90/10 split) requires careful consideration. Our weighted loss approach prevents the model from simply predicting the majority spoof class most of the time. However, this introduces a hyperparameter (the weight ratio) that must be tuned. We believe our approach is robust while acknowledging alternatives such as oversampling, undersampling, and focal loss could also be explored. 

\subsection{Generalization Considerations}

A key challenge in spoofing detection is generalization to unseen synthesis methods. The ASVspoof 2021 dataset contains multiple TTS and voice conversion systems, but new synthesis techniques are constantly emerging. Our data augmentation strategies aim to improve robustness, but evaluation on completely novel synthesis methods remains an important future direction.

\section{Conclusion}

This paper presented a comprehensive comparison of three deep learning architectures for detecting AI-generated speech. We implemented a CNN with attention, a Vision Transformer, and a Bidirectional LSTM, each operating on different feature representations (Mel-spectrograms and MFCCs). Through careful handling of class imbalance and appropriate data augmentation, we trained these models on the ASVspoof 2021 dataset.

Our analysis reveals the strengths and limitations of each approach:
\begin{itemize}
    \item CNNs excel at detecting local spectral artifacts through convolutional filters
    \item Vision Transformers capture global dependencies via self-attention mechanisms
    \item Bi-LSTMs model temporal dynamics and sequential patterns in acoustic features
\end{itemize}

The comparative evaluation provides insights into which architectural choices are most effective for this critical security task. As synthetic speech becomes increasingly sophisticated, robust detection methods are essential for protecting voice-based authentication systems, preventing fraud, and maintaining trust in audio content.

The code and models developed in this work are available to support reproducibility and future research in this important area.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
